{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import skimage\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import h5py as h5py\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import load_model\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'\n",
    "DATA_FILES = ['train-1.npy', 'train-2.npy', 'train-3.npy', 'train-4.npy']\n",
    "OUT_PATH = './data_binary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filenames, fixed_labels_dict={}, img_size=68, verbose=0, thresholding=0):\n",
    "    data = []\n",
    "    labels = []\n",
    "    ind = 0\n",
    "    for filename in filenames:\n",
    "        if verbose:\n",
    "            print('Processing file %s' %filename)\n",
    "        full_filename = DATA_PATH + filename\n",
    "        tmp_data = np.load(full_filename)\n",
    "        \n",
    "        for entry in tqdm(tmp_data):\n",
    "            img, label = entry\n",
    "            resized_img = resize(img, (img_size, img_size), mode='constant')\n",
    "            if (thresholding):\n",
    "                thresh = threshold_otsu(resized_img)\n",
    "                resized_img = resized_img <= thresh\n",
    "            data.append(resized_img)\n",
    "            \n",
    "            if label not in fixed_labels_dict:\n",
    "                fixed_labels_dict[label] = ind\n",
    "                ind += 1\n",
    "            labels.append(fixed_labels_dict[label])\n",
    "        del tmp_data\n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    if verbose:\n",
    "        print('Saving processed data in file')\n",
    "    np.savez(DATA_PATH + 'train_full', data = data, labels = labels)\n",
    "    np.save(DATA_PATH + 'fixed_labels', fixed_labels_dict)\n",
    "    del data, labels\n",
    "    gc.collect()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fixed_labels_dict = {}\n",
    "preprocess_data(DATA_FILES, fixed_labels_dict=fixed_labels_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load full dataset (requires around 12Gb RAM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATA_PATH + 'train_full.npz')\n",
    "images = data['data']\n",
    "labels = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_labels_dict = np.load(DATA_PATH + 'fixed_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_labels_dict = fixed_labels_dict[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reversed_dict(dictionary):\n",
    "    reversed_dict = {v: k for k, v in dictionary.items()}\n",
    "    return reversed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "    def init_model(self):\n",
    "        self.model.add(Conv2D(64, (3, 3), input_shape=(68, 68, 1), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(AveragePooling2D((2,2), strides=(2,2)))\n",
    "        self.model.add(BatchNormalization()) \n",
    "        \n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(4096, activation='relu'))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(4096, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(1000, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(optimizer=Adam(),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy']\n",
    "                          )\n",
    "    def print_model_info(self):\n",
    "        self.model.summary()\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def fit_model(self, x_train, x_valid, y_train, y_valid, batch_size=128, epochs=6):\n",
    "        print('Fitting model on train data...')\n",
    "        self.model.fit(x_train,\n",
    "                       y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=1,\n",
    "                       validation_data=(x_valid, y_valid)\n",
    "                      )\n",
    "        print('Saving model in file')\n",
    "        self.model.save('./models_data/model.h5')\n",
    "        \n",
    "        print('Fitting model on validation data...')\n",
    "        self.model.fit(x_valid,\n",
    "                       y_valid,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=1)\n",
    "        \n",
    "        print('Saving model in file')\n",
    "        self.model.save('./models_data/model.h5')\n",
    "        print('Successfully completed fitting process!')\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        self.predicton_labels = np.argmax(self.model.predict(test_data, verbose=1), axis=1)\n",
    "    \n",
    "    def get_predictions(self):\n",
    "        return self.predicton_labels\n",
    "        \n",
    "    def save_predictions_in_csv(self, reversed_labels_dict):\n",
    "        ind = 1\n",
    "        result = []\n",
    "        for label in self.predicton_labels:\n",
    "            result.append([ind, reversed_labels_dict[label]])\n",
    "            ind += 1\n",
    "        res_dataframe = pd.DataFrame(np.asarray(result))\n",
    "        res_dataframe.to_csv(DATA_PATH + 'predictions.csv', index=False, header=('Id', 'Category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_representation(data, num_classes=1000):\n",
    "    return keras.utils.to_categorical(data, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_final = images.reshape(images.shape[0], 68, 68, 1)\n",
    "labels_final = get_categorical_representation(labels)\n",
    "\n",
    "%xdel images\n",
    "%xdel labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires extra 12Gb of RAM (~25Gb total by now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(images_final, labels_final, test_size=0.15, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvNN()\n",
    "cnn.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 68, 68, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 68, 68, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 34, 34, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 34, 34, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 17, 17, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 63,844,520\n",
      "Trainable params: 63,842,600\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.print_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model on train data...\n",
      "Train on 283038 samples, validate on 49949 samples\n",
      "Epoch 1/7\n",
      "283038/283038 [==============================] - 1101s 4ms/step - loss: 2.7031 - acc: 0.4285 - val_loss: 1.2453 - val_acc: 0.7821\n",
      "Epoch 2/7\n",
      "283038/283038 [==============================] - 1065s 4ms/step - loss: 0.4898 - acc: 0.8663 - val_loss: 0.3441 - val_acc: 0.9072\n",
      "Epoch 3/7\n",
      "283038/283038 [==============================] - 1063s 4ms/step - loss: 0.2740 - acc: 0.9254 - val_loss: 0.2157 - val_acc: 0.9412\n",
      "Epoch 4/7\n",
      "283038/283038 [==============================] - 1060s 4ms/step - loss: 0.2070 - acc: 0.9438 - val_loss: 0.3829 - val_acc: 0.8989\n",
      "Epoch 5/7\n",
      "283038/283038 [==============================] - 1060s 4ms/step - loss: 0.1589 - acc: 0.9571 - val_loss: 0.1705 - val_acc: 0.9573\n",
      "Epoch 6/7\n",
      "283038/283038 [==============================] - 1060s 4ms/step - loss: 0.1312 - acc: 0.9647 - val_loss: 0.1209 - val_acc: 0.9703\n",
      "Epoch 7/7\n",
      "283038/283038 [==============================] - 1059s 4ms/step - loss: 0.1200 - acc: 0.9681 - val_loss: 0.0945 - val_acc: 0.9778\n",
      "Saving model in file\n",
      "Fitting model on validation data...\n",
      "Epoch 1/7\n",
      "49949/49949 [==============================] - 178s 4ms/step - loss: 0.1686 - acc: 0.9592\n",
      "Epoch 2/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.1007 - acc: 0.9744\n",
      "Epoch 3/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.0941 - acc: 0.9759\n",
      "Epoch 4/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.0705 - acc: 0.9813\n",
      "Epoch 5/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.0750 - acc: 0.9800\n",
      "Epoch 6/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.0699 - acc: 0.9821\n",
      "Epoch 7/7\n",
      "49949/49949 [==============================] - 177s 4ms/step - loss: 0.0685 - acc: 0.9826\n",
      "Saving model in file\n",
      "Successfully completed fitting process!\n"
     ]
    }
   ],
   "source": [
    "cnn.fit_model(x_train, x_valid, y_train, y_valid, batch_size=256, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = cnn.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting numpy objects from memory as they're no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24150"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%xdel x_train\n",
    "%xdel x_valid \n",
    "%xdel y_train\n",
    "%xdel y_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "332987/332987 [==============================] - 1136s 3ms/step - loss: 0.0582 - acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4326c86dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(images_final, labels_final, batch_size=512, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('./models_data/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "332987/332987 [==============================] - 1249s 4ms/step - loss: 0.0256 - acc: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c284658d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(images_final, labels_final, batch_size=1024, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('./models_data/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np = np.load(DATA_PATH + 'test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83247/83247 [01:14<00:00, 1120.37it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "for img in tqdm(test_data_np):\n",
    "    resized_img = resize(img, (68, 68), mode='constant')\n",
    "    #thresh = threshold_otsu(resized_img)\n",
    "    #binary_img = resized_img <= thresh\n",
    "    test_dataset.append(resized_img)\n",
    "test_dataset = np.asarray(test_dataset)\n",
    "test_dataset = test_dataset.reshape(test_dataset.shape[0], 68, 68, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83247/83247 [==============================] - 123s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_labels_dict = get_reversed_dict(fixed_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save_predictions_in_csv(reversed_labels_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
