{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "show": true
   },
   "source": [
    "# L4 – Word embedding\n",
    "\n",
    "### Resources\n",
    "1. Possible [truncated svd](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) implementation of sklearn.\n",
    "2. Tensorflow [tutorial](https://www.tensorflow.org/tutorials/word2vec) for word2vec and [implementation](https://github.com/tensorflow/models/tree/master/tutorials/embedding).\n",
    "3. GloVe [site](https://nlp.stanford.edu/projects/glove/).\n",
    "4. FastText [site](https://fasttext.cc).\n",
    "5. [Gensim](https://radimrehurek.com/gensim/) library.\n",
    "\n",
    "**Word embedding** – the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much higher dimensionality. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis and translation. Usually, problem reduces to unsupervised learning on a large corpus of text. So, learning process is based on the idea that context of word closely associated with it. For example, the context can be a document in which the word is located or adjacent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data\n",
    "\n",
    "As text corpus you may use simple english [wikipedia](https://simple.wikipedia.org/wiki/Simple_English_Wikipedia).\n",
    "1. 131K articles\n",
    "2. 2K common English words.\n",
    "\n",
    "Also you can use just english [wikipedia](https://en.wikipedia.org/wiki/Wikipedia):\n",
    "1. Includes 5М+ articles\n",
    "2. 2M+ different tokens\n",
    "\n",
    "#### Exercises\n",
    "1. Download all wikipedia articles.\n",
    "2. Make all preprocessing work that you need (e.g. remove markup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gensim\n",
    "import pattern\n",
    "import sklearn.decomposition\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from gensim.corpora import WikiCorpus, MmCorpus, Dictionary\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter out words which are found is less than 7 articles and those which are attributed to more than 40% of all articles in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-09 16:46:56,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-02-09 16:47:04,003 : INFO : adding document #10000 to Dictionary(145816 unique tokens: ['bankruptcy', 'donlin', 'suebians', 'transtromer', 'ot']...)\n",
      "2018-02-09 16:47:10,233 : INFO : adding document #20000 to Dictionary(207404 unique tokens: ['bankruptcy', 'donlin', 'suebians', 'woodsboro', 'praiseland']...)\n",
      "2018-02-09 16:47:17,919 : INFO : adding document #30000 to Dictionary(264020 unique tokens: ['bankruptcy', 'donlin', 'suebians', 'collamer', 'woodsboro']...)\n",
      "2018-02-09 16:47:26,104 : INFO : adding document #40000 to Dictionary(304936 unique tokens: ['bankruptcy', 'donlin', 'suebians', 'collamer', 'woodsboro']...)\n",
      "2018-02-09 16:47:32,796 : INFO : adding document #50000 to Dictionary(358223 unique tokens: ['bankruptcy', 'donlin', 'nothosaur', 'collamer', 'boucherie']...)\n",
      "2018-02-09 16:47:39,689 : INFO : adding document #60000 to Dictionary(411366 unique tokens: ['bankruptcy', 'donlin', 'nothosaur', 'collamer', 'fickman']...)\n",
      "2018-02-09 16:47:45,206 : INFO : adding document #70000 to Dictionary(431810 unique tokens: ['bankruptcy', 'donlin', 'nothosaur', 'collamer', 'fickman']...)\n",
      "2018-02-09 16:47:51,614 : INFO : adding document #80000 to Dictionary(457141 unique tokens: ['bankruptcy', 'donlin', 'nothosaur', 'collamer', 'fickman']...)\n",
      "2018-02-09 16:47:56,568 : INFO : finished iterating over Wikipedia corpus of 87384 documents with 23340546 positions (total 243051 articles, 24725459 positions before pruning articles shorter than 50 words)\n",
      "2018-02-09 16:47:56,659 : INFO : built Dictionary(484214 unique tokens: ['bankruptcy', 'donlin', 'nothosaur', 'collamer', 'fickman']...) from 87384 documents (total 23340546 corpus positions)\n",
      "2018-02-09 16:47:57,850 : INFO : discarding 415578 tokens: [('alvares', 3), ('an', 44251), ('and', 79263), ('aperire', 1), ('as', 47631), ('at', 39163), ('by', 47738), ('for', 52355), ('from', 46165), ('grounation', 2)]...\n",
      "2018-02-09 16:47:57,851 : INFO : keeping 68636 tokens which were in no less than 7 and no more than 34953 (=40.0%) documents\n",
      "2018-02-09 16:47:58,193 : INFO : resulting dictionary: Dictionary(68636 unique tokens: ['unsigned', 'bankruptcy', 'scaling', 'shrimps', 'ciénaga']...)\n",
      "2018-02-09 16:47:58,233 : INFO : saving Dictionary object under filtered_wiki.dict, separately None\n",
      "2018-02-09 16:47:58,283 : INFO : saved filtered_wiki.dict\n"
     ]
    }
   ],
   "source": [
    "DUMP_NAME = \"simplewiki-20180201-pages-articles.xml.bz2\"\n",
    "wiki_corpus = WikiCorpus(DUMP_NAME, lemmatize=False)\n",
    "wiki_corpus.dictionary.filter_extremes(no_below=7, no_above=0.4, keep_n=100000)\n",
    "wiki_corpus.dictionary.save('filtered_wiki.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/interfaces.py:60: UserWarning: corpus.save() stores only the (tiny) iteration object; to serialize the actual corpus content, use e.g. MmCorpus.serialize(corpus)\n",
      "  \"corpus.save() stores only the (tiny) iteration object; \"\n",
      "2018-02-09 16:56:14,530 : INFO : saving WikiCorpus object under simple_wiki_corpus, separately None\n",
      "2018-02-09 16:56:14,572 : INFO : saved simple_wiki_corpus\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus.save('simple_wiki_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSA (Latent semantic analysis)\n",
    "This solution uses full document as a context of word. So, we have some vocabulary $W$ and a set of documents $D$. Matrix $X$ with shape $|W| \\times |D|$ at position $w, d$ stores importance of word $w$ for document $d$. If word $w$ is not found in the document $d$ than at appropriate position $X$ has 0 (obviously, matrix is sparse).\n",
    "\n",
    "For each matrix you can find [SVD decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "$$X = U \\Sigma V^{T} \\text{, где }$$\n",
    "* $U$ – orthogonal matrix $|W| \\times |W|$ of left singular vectors\n",
    "* $\\Sigma$ – diagonal matrix $|W| \\times |D|$ of singular values\n",
    "* $V$ – orthogonal matrix $|D| \\times |D|$  of right singular vectors\n",
    "\n",
    "Let's suppouse that row $w$ in matrix $U\\Sigma$ is a vector that represents word $w$, and row $d$ of $V$ coresponds to document $d$. In some sense we already found the embeddings of words and documents at the same time. But size of vectors are determined by documents number $|D|$.\n",
    "\n",
    "Nevertheless you can use truncated SVD instead\n",
    "$$ X \\approx X_k = U_k \\Sigma_k V^{T}_k \\text{, where }$$\n",
    "* $U_k$ – $k$ left singular vectors\n",
    "* $\\Sigma_k$ – diagonal matrix $|k| \\times |k|$ of largest singular values\n",
    "* $V_k$ – $k$ right singular vectors\n",
    "\n",
    "Also it known that $X_k$ is the best approximation of $X$ in the term of [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) for all matrices of rank $k$.\n",
    "\n",
    "So, it is possible to obtain a compressed words' representations of size $k \\ll |W|$ as rows of matrix $U_k \\Sigma_k $. It just remains to understand how to calculate the original values of the matrix $X$. There is a set of approaches, here are some of them\n",
    "1. Binary flag that document contains the word.\n",
    "2. The number occurrences of word in document.\n",
    "3. Word frequency that is better known as $tf$ (it's possible also use $\\ln(1 + tf)$).\n",
    "4. [$tf \\cdot idf$](https://ru.wikipedia.org/wiki/TF-IDF).\n",
    "\n",
    "#### Further readning\n",
    "1. You also can read this [aticle](https://en.wikipedia.org/wiki/Latent_semantic_analysis).\n",
    "2. By the way, some [modification](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) of SVD decompostion won at [Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize).\n",
    "3. It is easy to see that the problem of SVD decomposition is reduced to finding eigenvectors of matrix $X \\cdot X^T$.\n",
    "4. You can find [here](https://arxiv.org/abs/0909.4061) probabilistic algorithms of matrix decomposition.\n",
    "\n",
    "#### Exercises\n",
    "1. Find matrix $X$, using you favourite approach (maybe you need the sparse matrix class of scipy library).\n",
    "2. Find word embeddings of size $k = 128$.\n",
    "3. Implement k-nearest neighbor search for Euclidean norm.\n",
    "4. Find 10 nearest words for the set of words: 'cat', 'loves', 'clever' and 'mandarin'.\n",
    "5. Repeat experiment for cosinus as metric. What are the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly find $X$ matrix by using tf-idf approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(wiki_corpus, id2word=wiki_corpus.dictionary)\n",
    "wiki_tfidf = tfidf[wiki_corpus]\n",
    "wiki_sparse = gensim.matutils.corpus2csc(wiki_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have sparse matrix, we can find word embeddings by reducing dimensionality to $R^{128}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = sklearn.decomposition.TruncatedSVD(n_components=DIM, n_iter=7, random_state=1773)\n",
    "embeddings = svd.fit_transform(wiki_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save embeddings and sparse matrix in order to use them when reloading kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('wiki_sparse_tfidf.pickle', 'wb') as f:\n",
    "    pickle.dump(wiki_sparse, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to implement kNN and play with words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest(word, embeddings, dictionary, k, metric):\n",
    "    def id2word(idx):\n",
    "        return dictionary[idx]\n",
    "    \n",
    "    def word2id(word):\n",
    "        return dictionary.doc2idx([word])[0]\n",
    "    \n",
    "    distances = cdist(embeddings, \n",
    "                      embeddings[word2id(word)].reshape(1, embeddings.shape[1]),\n",
    "                      metric=metric).flatten()\n",
    "        \n",
    "    return [(id2word(x), distances[x]) for x in np.argsort(distances)[:k + 1] if x != word2id(word)][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_nearest(word, embeddings, dictionary, k, metric):\n",
    "    nearest_list = get_k_nearest(word, embeddings, dictionary, k, metric)\n",
    "    print('Nearest to', word)\n",
    "    print('%15s %10s' %('word', 'distance'))\n",
    "    print('='*30)\n",
    "    for word_info in nearest_list:\n",
    "        print(\"%15s | %.4f\" %(word_info[0], word_info[1]))\n",
    "    print('='*30, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to cat\n",
      "           word   distance\n",
      "==============================\n",
      "           cats | 0.3558\n",
      "            dog | 0.4080\n",
      "          breed | 0.4174\n",
      "           bear | 0.4245\n",
      "         spider | 0.4298\n",
      "           dogs | 0.4322\n",
      "           eyes | 0.4328\n",
      "            fur | 0.4356\n",
      "           duck | 0.4549\n",
      "     appearance | 0.4558\n",
      "==============================\n",
      "\n",
      "Nearest to loves\n",
      "           word   distance\n",
      "==============================\n",
      "          funny | 0.1129\n",
      "          likes | 0.1129\n",
      "           ugly | 0.1254\n",
      "            mom | 0.1260\n",
      "      wonderful | 0.1260\n",
      "           sees | 0.1275\n",
      "           wish | 0.1280\n",
      "          knows | 0.1289\n",
      "          proud | 0.1297\n",
      "          curse | 0.1308\n",
      "==============================\n",
      "\n",
      "Nearest to clever\n",
      "           word   distance\n",
      "==============================\n",
      "        realize | 0.0395\n",
      "        jealous | 0.0405\n",
      "         honest | 0.0435\n",
      "       persuade | 0.0435\n",
      "     frightened | 0.0436\n",
      "          wakes | 0.0441\n",
      "     dostoevsky | 0.0442\n",
      "        shocked | 0.0444\n",
      "          cared | 0.0445\n",
      "         liking | 0.0446\n",
      "==============================\n",
      "\n",
      "Nearest to mandarin\n",
      "           word   distance\n",
      "==============================\n",
      "     simplified | 0.1181\n",
      "      cantonese | 0.1386\n",
      "      taiwanese | 0.1669\n",
      "           chen | 0.1814\n",
      "      guangdong | 0.1815\n",
      "          macau | 0.1842\n",
      "             yu | 0.1844\n",
      "          jiang | 0.1851\n",
      "          zhang | 0.1858\n",
      "         chiang | 0.1863\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_k_nearest('cat', embeddings, wiki_corpus.dictionary, 10, 'euclidean')\n",
    "print_k_nearest('loves', embeddings, wiki_corpus.dictionary, 10, 'euclidean')\n",
    "print_k_nearest('clever', embeddings, wiki_corpus.dictionary, 10, 'euclidean')\n",
    "print_k_nearest('mandarin', embeddings, wiki_corpus.dictionary, 10, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change metric to cosine and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to cat\n",
      "           word   distance\n",
      "==============================\n",
      "           cats | 0.1244\n",
      "            dog | 0.1496\n",
      "            pet | 0.1554\n",
      "        kittens | 0.1598\n",
      "           paws | 0.1783\n",
      "          tabby | 0.1957\n",
      "           ears | 0.1993\n",
      "         rabbit | 0.2100\n",
      "       whiskers | 0.2102\n",
      "        siamese | 0.2176\n",
      "==============================\n",
      "\n",
      "Nearest to loves\n",
      "           word   distance\n",
      "==============================\n",
      "         friend | 0.1268\n",
      "        friends | 0.1354\n",
      "          happy | 0.1744\n",
      "           ugly | 0.1892\n",
      "          funny | 0.1900\n",
      "          likes | 0.1904\n",
      "           girl | 0.2024\n",
      "            mom | 0.2061\n",
      "       realises | 0.2061\n",
      "          proud | 0.2093\n",
      "==============================\n",
      "\n",
      "Nearest to clever\n",
      "           word   distance\n",
      "==============================\n",
      "         really | 0.1686\n",
      "         thinks | 0.1713\n",
      "        telling | 0.1714\n",
      "          knows | 0.1721\n",
      "          angry | 0.1810\n",
      "        jealous | 0.1847\n",
      "        realize | 0.1995\n",
      "          proud | 0.2014\n",
      "       dislikes | 0.2068\n",
      "            sad | 0.2099\n",
      "==============================\n",
      "\n",
      "Nearest to mandarin\n",
      "           word   distance\n",
      "==============================\n",
      "      cantonese | 0.0334\n",
      "     simplified | 0.0863\n",
      "        hokkien | 0.0991\n",
      "             mǎ | 0.1026\n",
      "            rén | 0.1152\n",
      "          hakka | 0.1195\n",
      "    proficiency | 0.1201\n",
      "        chinese | 0.1326\n",
      "       tungusic | 0.1501\n",
      "         pinyin | 0.1637\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_k_nearest('cat', embeddings, wiki_corpus.dictionary, 10, 'cosine')\n",
    "print_k_nearest('loves', embeddings, wiki_corpus.dictionary, 10, 'cosine')\n",
    "print_k_nearest('clever', embeddings, wiki_corpus.dictionary, 10, 'cosine')\n",
    "print_k_nearest('mandarin', embeddings, wiki_corpus.dictionary, 10, 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, cosine metric performs a little bit better meaning that nearest words are generally more relevant, comparing to results with euclidean metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. word2vec\n",
    "Let's consider perhaps the most popular model of word embeddings. This is mainly due to the fact that the received vectors have interesting properties. In particular, semantically similar words have close vectors, moreover linear operation of subtraction has meaning in the resulting vector space! The context of word at this time is a window of size $2c+1$, where interested word is located in the middle.\n",
    "\n",
    "#### Continuous bag-of-words\n",
    "The core idea is very simple. There is some very long text\n",
    "$$w_1, w_2, \\dots, w_T.$$\n",
    "Consider some word $w_t$ and its context in radius $c$, namely words\n",
    "$$w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots w_{t+c}.$$\n",
    "Intuition tells us that the context often really well defines the word in the middle. Then let our model restore the central word if its context is known\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{t} \\log \\mathbb{P}[w_t|w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots w_{t+c}].$$\n",
    "\n",
    "For each word $w_t$ there is some vector $v_t$, but if word $w_{t+i}$ is some context than we use vector $v'_{t+i}$. In this case we can represent context of word as the following sum\n",
    "$$s'_t = \\sum_{c \\leq i \\leq c, i \\neq 0} v'_{t+i}$$\n",
    "\n",
    "And define probabilty with softmax\n",
    "$$\\mathbb{P}[w_t|w_{t-c}, \\dots, w_{t-1}, w_{t-1}, \\dots w_{t+c}] = \\frac{ \\exp {s'}_t^T \\cdot v_t}{\\sum_j \\exp {s'}_t^T \\cdot v_j}.$$\n",
    "\n",
    "Note that this model is easy to present as 2-layer neural network:\n",
    "* The input is a sparse vector of dimension $|W|$ which at position $w'$ has one if word $w'$ is in context.\n",
    "* Further, we have matrix $V'$ of vectors $v'$.\n",
    "* Further, without any non-linearity matrix $V$ of vector $v$.\n",
    "* And then the softmax layer.\n",
    "\n",
    "#### Skip-gram\n",
    "Another model predicts context for some word. The design is approximately the same, we need to optimize\n",
    "$$\\mathcal{L} = -\\frac{1}{T} \\sum_t \\sum_{-c \\leq i \\leq c, i \\neq 0} \\mathbb{P}[w_{t+i}|w_t].$$\n",
    "\n",
    "The probability is approximated by the following model\n",
    "$$\\mathbb{P}[w_{t+i}|w_t] = \\frac{ \\exp {v'}_{t+i}^T \\cdot v_t}{\\sum_j \\exp {v'}_{j}^T \\cdot v_t}.$$\n",
    "\n",
    "\n",
    "#### Hierarchical softmax\n",
    "Creation of this mode was guided to reduce the complexity of learning. However, computing of sofmax's denominator is really expensive operation. Therefore, in practice, a few other models are used.\n",
    "\n",
    "This method was viewed in the seminar. Detailed information can be found [here](http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf) and practical using [here](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).    Briefly recall, for each word from the dictionary we define [Huffman code](https://en.wikipedia.org/wiki/Huffman_coding). More frequency words have shorter code. Then we construct the Huffman tree. In accordance with its code words are stored in leaves. In the non-leaf vertices are stored special hidden representations $h'$. \n",
    "\n",
    "$$\\mathbb{P}[w_{t+i}|w_t] = \\prod_{{h'}_{t+i}, r} \\sigma\\Big( \\mathcal{r} \\cdot {h'}^T_{t+i} v_t \\Big) \\text{, where}$$\n",
    "* ${h'}_{t+i}$ - hidden vector of word's path $w_{t+i}$\n",
    "* $\\mathcal{r}$ - equals $-1$, if we turn left and $+1$ in another case\n",
    "* $\\sigma$ - sigmoid function\n",
    "\n",
    "Using hierarchical softmax as approximation of softmax allows us to significantly reduce complexity of model training. \n",
    "\n",
    "#### Exercise\n",
    "1. How can you implement skip-gram model as neural network.\n",
    "2. Estimate complexity of one iteration of training the skip-gram model\n",
    "  * $T$ - text size\n",
    "  * $W$ - vocabulary size\n",
    "  * $c$ - context radius\n",
    "  * $d$ - embedding size\n",
    "3. Estimate complexity of using hierarchical softmax in the skip-gram model.\n",
    "\n",
    "#### Why sampling?\n",
    "Let see at softmax funciton\n",
    "$$\\mathbb{P}[x_i] = \\frac{\\exp f(x_i|\\theta)}{\\sum_j \\exp f(x_j|\\theta)}$$\n",
    "Optimizing this function is equivalent to optimizing its logarithm\n",
    "$$\\log \\frac{\\exp f(x_i|\\theta)}{\\sum_j \\exp f(x_j|\\theta)} = f(x_i|\\theta) - log \\sum_j \\exp f(x_j|\\theta)$$\n",
    "Than gradient $\\triangledown_{\\theta} \\mathbb{P}[x_i]$ is equal to\n",
    "$$\\triangledown_{\\theta} f(x_i|\\theta) - \\sum_k \\frac{\\exp f(x_k|\\theta)}{\\sum_j \\exp f(w_j|\\theta)} \\triangledown_{\\theta} f(x_k|\\theta)$$\n",
    "Inside the sum we can find $\\mathbb{P}[x_k]$ again. So, the previous expression can be written as\n",
    "$$\n",
    "\\triangledown_{\\theta} f(x_i|\\theta) - \\sum_k \\mathbb{P}[x_k] \\cdot \\triangledown_{\\theta} f(x_k|\\theta) = \n",
    "\\triangledown_{\\theta} f(x_i|\\theta) - \\mathbb{E}_{x \\sim \\mathbb{P}[x]} \\triangledown_{\\theta} f(x|\\theta)\n",
    "$$\n",
    "If we can sample $x \\sim \\mathbb{P}[x]$, then we has no sense to calculate the softmax itself. We just can train model, computing only gradient. It is possible due to the fact that we do not need the probability itself, but want to find representation of words only.\n",
    "\n",
    "#### Negative Sampling\n",
    "As the result, in practice, softmax loss functions is approximated with negative sampling, which looks like a series of logistic regressions\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{w_t}\n",
    "    \\sum_{-c \\leq i \\leq c, i \\neq 0} \\Big(\n",
    "        \\mathbb{P}[y = 1 | w_{t+i}, w_t] -\n",
    "        k \\cdot \\mathbb{E}_{\\tilde{w}_{tj} \\sim \\mathbb{P}[w_j|w_t]} \\mathbb{P}[y = 0 | \\tilde{w}_{tj}, w_t]\n",
    "    \\Big)\n",
    "$$\n",
    "where $y=1$, if  word is a context of word and $y=0$ in another case. А $\\tilde{w}_{tj}$ – sampling of words that don't belong to the context. Then main trick is how we will sample words $\\mathbb{P}[y | w_{t+i}, w_t]$. In paper [noise contrastive estimation](https://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf) some interesting approaches is suggested. But we are going to use more simple variation\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{w_t} \\Big(\n",
    "    \\sum_{-c \\leq i \\leq c, i \\neq 0} \\big(\n",
    "        \\sigma({v'}_{t+i}^T v_t) +\n",
    "        k \\cdot \\mathbb{E}_{w_j \\sim \\mathbb{Q}(w_j)} \\sigma (-{v'}_{j}^T v_t)\n",
    "    \\big)\n",
    "\\Big),\n",
    "$$\n",
    "to find  $\\mathbb{E}_{\\tilde{w}_{tj} \\sim \\mathbb{P}[w_j|w_t]}$ we simply sample words $k$ times from distribution $\\mathbb{Q}$, $\\mathbb{P}(w_j|w_t) = \\mathbb{Q}(w_j)$.\n",
    "\n",
    "#### Additional word2vec tricks\n",
    "For the best quality at the moment of learning it is proposed to use variety of tricks:\n",
    "1. How to choose $\\mathbb{Q}$.\n",
    "2. Sampling of frequent and rare words.\n",
    "3. The sampling of window size.\n",
    "You are welcome to read about this in original papers.\n",
    "\n",
    "#### Further reading\n",
    "1. Briefly about both models [here](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "2. Hierarchical softmax and negative sampling [here](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "#### Exercises\n",
    "1. Train skip-gram model, using negative sampling and embedding size $d$ = 256. You are free to choose size of window and number of epochs.\n",
    "2. Find top-10 nearest words again. Compare Euclidean and cosinus metric. Which is better? \n",
    "3. Find 10 nearest vector for expression v(king) - v(man) + v(women). If your model is tuned well, you find representation of word queen. \n",
    "4. How could you solve the following problems?\n",
    "  * Find for country its capital\n",
    "  * For a word find its plural form\n",
    "  * For word define antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-09 17:01:09,163 : INFO : loading Dictionary object from filtered_wiki.dict\n",
      "2018-02-09 17:01:09,219 : INFO : loaded filtered_wiki.dict\n"
     ]
    }
   ],
   "source": [
    "filtered_voc = Dictionary.load('filtered_wiki.dict')\n",
    "vocabulary_set = set(filtered_voc.values())\n",
    "vocabulary_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(corpus, vocabulary):\n",
    "    words = []\n",
    "    \n",
    "    print('INFO: extracting words from corpus')\n",
    "    for text in tqdm(corpus.get_texts()):\n",
    "        words.extend([word for word in text if word in vocabulary])\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common())\n",
    "    dictionary = dict()\n",
    "    \n",
    "    print('INFO: creating dictionary')\n",
    "    for word, _ in tqdm(count):\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    data = list()\n",
    "    print('INFO: creating dataset')\n",
    "    for word in tqdm(words):\n",
    "        index = dictionary[word]\n",
    "        data.append(index)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: extracting words from corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87313it [01:05, 1331.38it/s]2018-02-09 17:25:01,281 : INFO : finished iterating over Wikipedia corpus of 87384 documents with 23340546 positions (total 243051 articles, 24725459 positions before pruning articles shorter than 50 words)\n",
      "87384it [01:05, 1329.88it/s]\n",
      "100%|██████████| 68636/68636 [00:00<00:00, 994514.94it/s]\n",
      "  1%|          | 157554/16506837 [00:00<00:10, 1575075.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: creating dictionary\n",
      "INFO: creating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16506837/16506837 [00:09<00:00, 1716552.38it/s]\n"
     ]
    }
   ],
   "source": [
    "t_data, t_count, t_dict, t_rev_dict = build_dataset(wiki_corpus, vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        \n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "            \n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the whole corpus in words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16506837"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "embedding_size = 256\n",
    "skip_window = 4\n",
    "num_skips = 8\n",
    "num_sampled = 64\n",
    "\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "w2vec_graph = tf.Graph()\n",
    "\n",
    "with w2vec_graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(tf.truncated_normal(\n",
    "            [vocabulary_size, embedding_size],\n",
    "            stddev=1.0 / np.sqrt(embedding_size)))\n",
    "        \n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "        \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 8\n",
    "num_steps = int((len(t_data) * num_skips * EPOCHS) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/2063354 [00:00<5:23:33, 106.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized\n",
      "Average loss at step  0 :  328.74310302734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 100032/2063354 [07:35<2:29:09, 219.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  100000 :  13.234589516444803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 200026/2063354 [14:57<2:19:23, 222.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  200000 :  5.177083097546958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 300043/2063354 [22:23<2:11:36, 223.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  300000 :  4.796784028993249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 400021/2063354 [31:02<2:09:04, 214.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  400000 :  4.670426457842886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 500027/2063354 [39:59<2:05:00, 208.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  500000 :  4.569717774108015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 600038/2063354 [48:21<1:57:57, 206.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  600000 :  4.522677504060268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 700032/2063354 [56:29<1:50:00, 206.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  700000 :  4.4464625632281605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 800023/2063354 [1:05:16<1:43:04, 204.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  800000 :  4.402198337654769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 900028/2063354 [1:13:25<1:34:53, 204.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  900000 :  4.45525150635317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1000036/2063354 [1:21:27<1:26:36, 204.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1000000 :  4.373270752547383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1100026/2063354 [1:30:41<1:19:25, 202.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1100000 :  4.375627747436911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1200035/2063354 [1:39:46<1:11:46, 200.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1200000 :  4.35435207176283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1300020/2063354 [1:48:49<1:03:54, 199.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1300000 :  4.305400174218044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1400034/2063354 [1:58:47<56:16, 196.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1400000 :  4.345015908095241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1500022/2063354 [2:08:19<48:11, 194.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1500000 :  4.306146601245552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1600035/2063354 [2:17:24<39:47, 194.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1600000 :  4.301665123692155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1700031/2063354 [2:26:30<31:18, 193.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1700000 :  4.32476052629903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1800021/2063354 [2:36:29<22:53, 191.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1800000 :  4.247732371003852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1900033/2063354 [2:45:54<14:15, 190.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1900000 :  4.313810577608943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2000025/2063354 [2:54:47<05:32, 190.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2000000 :  4.248996219328344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063354/2063354 [3:00:23<00:00, 190.63it/s]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=w2vec_graph) as session:\n",
    "    init.run()\n",
    "    print('Successfully initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        batch_inputs, batch_labels = generate_batch(t_data, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss],\n",
    "                              feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 100000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 100000\n",
    "            # The average loss is an estimate of the loss over the last 100000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "    final_embeddings_norm = normalized_embeddings.eval()\n",
    "    final_embeddings = embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models in order to use it after restarting kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('w2v_norm_embed', final_embeddings_norm)\n",
    "np.save('w2v_embed', final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nearest(word, embeddings, word2id_dict, id2word_dict, k, metric):\n",
    "    def _get_k_nearest():\n",
    "        def id2word(idx):\n",
    "            return id2word_dict[idx]\n",
    "    \n",
    "        def word2id(word):\n",
    "            return word2id_dict[word]\n",
    "    \n",
    "        distances = cdist(embeddings, \n",
    "                          embeddings[word2id(word)].reshape(1, embeddings.shape[1]),\n",
    "                          metric=metric).flatten()\n",
    "        \n",
    "        return [(id2word(x), distances[x]) for x in np.argsort(distances)[:k + 1] if x != word2id(word)][:k]\n",
    "\n",
    "    nearest_list = _get_k_nearest()\n",
    "    print('Nearest to', word)\n",
    "    print('%15s %10s' %('word', 'distance'))\n",
    "    print('='*30)\n",
    "    for word_info in nearest_list:\n",
    "        print(\"%15s | %.4f\" %(word_info[0], word_info[1]))\n",
    "    print('='*30, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to cat\n",
      "           word   distance\n",
      "==============================\n",
      "           cats | 0.9190\n",
      "            dog | 0.9489\n",
      "          breed | 0.9914\n",
      "            pet | 1.0403\n",
      "          mouse | 1.0472\n",
      "           face | 1.0700\n",
      "           bear | 1.0775\n",
      "       domestic | 1.0814\n",
      "         monkey | 1.0823\n",
      "      character | 1.0834\n",
      "==============================\n",
      "\n",
      "Nearest to loves\n",
      "           word   distance\n",
      "==============================\n",
      "          knows | 0.9664\n",
      "          likes | 0.9667\n",
      "           love | 0.9823\n",
      "          wants | 0.9910\n",
      "         thinks | 0.9933\n",
      "          loved | 0.9996\n",
      "          happy | 1.0162\n",
      "           tell | 1.0171\n",
      "          hates | 1.0249\n",
      "         loving | 1.0315\n",
      "==============================\n",
      "\n",
      "Nearest to clever\n",
      "           word   distance\n",
      "==============================\n",
      "      beautiful | 1.0365\n",
      "    intelligent | 1.0690\n",
      "          proud | 1.0744\n",
      "          liked | 1.0761\n",
      "           hero | 1.0864\n",
      "          funny | 1.0912\n",
      "          loyal | 1.0913\n",
      "         pretty | 1.0936\n",
      "            fun | 1.0964\n",
      "       familiar | 1.0996\n",
      "==============================\n",
      "\n",
      "Nearest to mandarin\n",
      "           word   distance\n",
      "==============================\n",
      "        chinese | 0.9301\n",
      "      cantonese | 1.0258\n",
      "         pinyin | 1.0311\n",
      "             wu | 1.0506\n",
      "             li | 1.0516\n",
      "      taiwanese | 1.0665\n",
      "       language | 1.0872\n",
      "        dialect | 1.0879\n",
      "         spoken | 1.0886\n",
      "        beijing | 1.0931\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_nearest('cat', final_embeddings_norm, t_dict, t_rev_dict, 10, 'euclidean')\n",
    "print_nearest('loves', final_embeddings_norm, t_dict, t_rev_dict, 10, 'euclidean')\n",
    "print_nearest('clever', final_embeddings_norm, t_dict, t_rev_dict, 10, 'euclidean')\n",
    "print_nearest('mandarin', final_embeddings_norm, t_dict, t_rev_dict, 10, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to cat\n",
      "           word   distance\n",
      "==============================\n",
      "           cats | 0.4222\n",
      "            dog | 0.4502\n",
      "          breed | 0.4915\n",
      "            pet | 0.5411\n",
      "          mouse | 0.5483\n",
      "           face | 0.5725\n",
      "           bear | 0.5805\n",
      "       domestic | 0.5847\n",
      "         monkey | 0.5857\n",
      "      character | 0.5869\n",
      "==============================\n",
      "\n",
      "Nearest to loves\n",
      "           word   distance\n",
      "==============================\n",
      "          knows | 0.4670\n",
      "          likes | 0.4672\n",
      "           love | 0.4825\n",
      "          wants | 0.4910\n",
      "         thinks | 0.4933\n",
      "          loved | 0.4996\n",
      "          happy | 0.5163\n",
      "           tell | 0.5172\n",
      "          hates | 0.5252\n",
      "         loving | 0.5320\n",
      "==============================\n",
      "\n",
      "Nearest to clever\n",
      "           word   distance\n",
      "==============================\n",
      "      beautiful | 0.5371\n",
      "    intelligent | 0.5714\n",
      "          proud | 0.5771\n",
      "          liked | 0.5790\n",
      "           hero | 0.5902\n",
      "          funny | 0.5953\n",
      "          loyal | 0.5955\n",
      "         pretty | 0.5980\n",
      "            fun | 0.6011\n",
      "       familiar | 0.6045\n",
      "==============================\n",
      "\n",
      "Nearest to mandarin\n",
      "           word   distance\n",
      "==============================\n",
      "        chinese | 0.4326\n",
      "      cantonese | 0.5262\n",
      "         pinyin | 0.5316\n",
      "             wu | 0.5519\n",
      "             li | 0.5530\n",
      "      taiwanese | 0.5687\n",
      "       language | 0.5910\n",
      "        dialect | 0.5917\n",
      "         spoken | 0.5925\n",
      "        beijing | 0.5974\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_nearest('cat', final_embeddings_norm, t_dict, t_rev_dict, 10, 'cosine')\n",
    "print_nearest('loves', final_embeddings_norm, t_dict, t_rev_dict, 10, 'cosine')\n",
    "print_nearest('clever', final_embeddings_norm, t_dict, t_rev_dict, 10, 'cosine')\n",
    "print_nearest('mandarin', final_embeddings_norm, t_dict, t_rev_dict, 10, 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. GloVe\n",
    "This model combines two ideas:\n",
    "1. Factorization of the matrix, as in LSA\n",
    "2. Context of word is adjacent words in the corpus, just in word2vec.\n",
    "\n",
    "Let matrix $X$ at position $i,j$ stores how many times the word $j$ was found in the context of word $i$. This matrix is filled with our corpus of text. Then the probability that the word $j$ appears in the context of word $i$ is equal to\n",
    "$$P_{ij} = \\frac{X_{ij}}{\\sum_k X_{ik}}$$\n",
    "\n",
    "At the same time we want to find some function $F$ and word embeddings that \n",
    "$$F((w_i - w_j)^T w_k) = \\frac{F(w_i^T w_k)}{F(w_j^T w_k)} = \\frac{P_{ik}}{P_{jk}}.$$\n",
    "About motivation it's better to read in the original [paper](http://nlp.stanford.edu/pubs/glove.pdf).\n",
    "Possible solution may be following\n",
    "$$\\exp(w_i^T w_j) = P_{ij}.$$\n",
    "\n",
    "After a series of transformations you can receive that\n",
    "$$w_i^T w_j = \\log X_{ij} - \\log\\big(\\sum_k X_{ik}\\big)$$\n",
    "\n",
    "Obviously, right part doesn't depend on $j$, but the resulting model is offered to be written symmetry as\n",
    "$$w_i^T w_j + b_i + b_j = \\log X_{ij}$$\n",
    "\n",
    "As loss function authors suggest to use weighted MSE\n",
    "$$\\mathcal{L} = \\sum_{i,j = 1}^{|W|} f(X_{ij}) \\Big(w_i^T w_j + b_i + b_j - \\log(X_{ij}) \\Big)^2,$$\n",
    "where $f$ is some weight function, defined for each pair of words. It is reasonable to store $X$ as sparse matrix. It is also noteworthy that during training only **not null** values of $X$ used. Model is trained with stochastic gradient descent. As some way of regularization the authors suggest to use two different matrices $W$ и $W'$ for word and context, so model can be rewritten as\n",
    "$$w_i^T {w'}_j + b_i + b_j = \\log X_{ij}$$\n",
    "The resulted embedding of a word is the sum of two trained views $w + w'$.\n",
    "\n",
    "#### Some additional GloVe tricks\n",
    "1. In order to understand how you can choose $f$, please, read the original paper.\n",
    "2. Also you are welcome to see, how $X$ is formed in the original article.\n",
    "\n",
    "#### Further reading\n",
    "1. GloVe [site](http://nlp.stanford.edu/projects/glove/).\n",
    "2. Some words about [t-SNE](http://lvdmaaten.github.io/tsne/).\n",
    "\n",
    "#### Exercises\n",
    "1. Estimate complexity of model for one iteration. Choose appropriate $c$ for you.\n",
    "2. Train GloVe word embeddings ($d$=256).\n",
    "3. Check that v(king) - v(man) + v(women) is approximately equal v(queen).\n",
    "4. Read about [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "5. Use t-SNE to reduce the size of embeddings to 3. Make sure that the following groups of vectors are collinear (use visualization)\n",
    "  * [man, woman], [Mr., Ms], [king, queen], etc\n",
    "  * [CEO, company]\n",
    "  * [adjective, its comparative form]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. FastText\n",
    "[FastText](https://arxiv.org/pdf/1607.04606.pdf) is a logical development of skip-gram model. The core feature is to present the word as the sum of its n-grams embeddings and its own embedding vector. For example, if we want to use only 3-grams, than word **fast** be reprented as n-grams **-fa**, **fas**, **ast**, **st-** and word **-fast-**. The context is encoeded in usual way, as a result similarity of word and context word is the following sum\n",
    "$$s(w, c) = \\sum_{n \\in N_w} z^{T}_n v'_{c},$$\n",
    "where $N_w$ is the set of all n-grams of word $w$ combined with word itself. The authors argue that the use of combination of 3-,4- and 5-grams helps to get better embeddings for rare and even unknown words (emdedding of word itself is null vector). For model training is proposed to use negative sampling.\n",
    "\n",
    "You can find more information on [site](https://fasttext.cc) of FastText.\n",
    "\n",
    "#### Exercises\n",
    "1. Train FastText word embeddings ($d$=256).\n",
    "2. Find some rare words in your corpus and find top-5 nearest words for them, using cosinus as metric.\n",
    "3. Compare results with classic word2vec.\n",
    "4. Invent some funny new word and found its top-5 nearest words again.\n",
    "5. How could you compare all models. Suggest your metric (see papers for inspiration)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
